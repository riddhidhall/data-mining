{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Focuses on app_train only\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from utils import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n",
      "ONE HOT ENCODED\n",
      "Training Features shape:  (307511, 243)\n",
      "Testing Features shape:  (48744, 239)\n",
      "ALIGNED:\n",
      "Training Features shape:  (307511, 240)\n",
      "Testing Features shape:  (48744, 239)\n",
      "There were 9274 anomalies in the test data out of 48744 entries\n",
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 240)\n",
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 231)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do basic pre-processing\n",
    "def pre_process():\n",
    "    # read data\n",
    "    train_data = load_training_data()\n",
    "    test_data = load_test_data()\n",
    "    train_data, test_data = encode_binary_cols(train_data, test_data)\n",
    "    train_data, test_data = one_hot_encode(train_data, test_data)\n",
    "    train_data, test_data, train_Y = align_data(train_data, test_data)\n",
    "    train_data, test_data = remove_days_employed_anomaly(train_data, test_data)\n",
    "    train_data, test_data = remove_missing_cols(train_data, test_data)\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_impute(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "train_data = mean_impute(train_data)\n",
    "test_data = mean_impute(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(train, test):\n",
    "    # MAKE SURE SK_CURR_ID AND TARGET have been dropped\n",
    "    # Normalise\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Scale each feature to 0-1\n",
    "    scaler.fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "    print(\"NORMALISED:\")\n",
    "    print('Training data shape: ', train.shape)\n",
    "    print('Testing data shape: ', test.shape)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_basic(train_X:np.array, train_Y, test_X, feature_names, model_save_path, n_folds=5):\n",
    "    print('Training Data Shape: ', train_X.shape)\n",
    "    print('Testing Data Shape: ', test_X.shape)\n",
    "    \n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_X.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(train_X.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "     # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(train_X):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = train_X[train_indices], train_Y[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = train_X[valid_indices], train_Y[valid_indices]\n",
    "        \n",
    "         # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances - get the average from all the splits in a fold\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions - get the average from all the folds\n",
    "        test_predictions += model.predict_proba(test_X, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions \n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Save model - will keep overwriting so saves last model in the end\n",
    "        save_pickle(model_save_path, model)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "            \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(train_Y, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return test_predictions, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNED:\n",
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 231)\n",
      "Training Data Shape:  (307511, 230)\n",
      "Testing Data Shape:  (48744, 230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumika\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype bool, uint8, int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALISED:\n",
      "Training data shape:  (307511, 230)\n",
      "Testing data shape:  (48744, 230)\n"
     ]
    }
   ],
   "source": [
    "# gbm run for features\n",
    "train_X, test_X , train_Y= align_data(train_data, test_data)\n",
    "train_X = train_X.drop(columns=['TARGET'])\n",
    "train_X = train_X.drop(columns=['SK_ID_CURR'])\n",
    "test_X = test_X.drop(columns=['SK_ID_CURR'])\n",
    "print('Training Data Shape: ', train_X.shape); print('Testing Data Shape: ', test_X.shape)\n",
    "feature_names = train_X.columns\n",
    "train_X, test_X = normalise(train_X, test_X)\n",
    "\n",
    "# test_Y, feature_importances, score_metrics = gbm_basic(train_X, train_Y, test_X, feature_names, \n",
    "#                                                        model_save_path=\"../models/gbm_features_basic.pickle\" ,n_folds=5)\n",
    "# create_and_save_submission(test_imputed, test_Y, save_path='../test_predictions/gbm_features_basic.csv')\n",
    "# save_pickle(\"../misc/gbm_simple_feature_importances.pickle\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EXT_SOURCE_1' 'EXT_SOURCE_3' 'EXT_SOURCE_2' 'DAYS_BIRTH' 'AMT_CREDIT'\n",
      " 'AMT_ANNUITY' 'DAYS_EMPLOYED' 'DAYS_ID_PUBLISH' 'AMT_GOODS_PRICE'\n",
      " 'DAYS_LAST_PHONE_CHANGE' 'DAYS_REGISTRATION' 'REGION_POPULATION_RELATIVE'\n",
      " 'AMT_INCOME_TOTAL' 'OWN_CAR_AGE' 'HOUR_APPR_PROCESS_START'\n",
      " 'TOTALAREA_MODE' 'AMT_REQ_CREDIT_BUREAU_YEAR' 'BASEMENTAREA_MODE'\n",
      " 'APARTMENTS_MODE' 'NAME_CONTRACT_TYPE' 'AMT_REQ_CREDIT_BUREAU_QRT'\n",
      " 'LANDAREA_MODE' 'YEARS_BEGINEXPLUATATION_MODE'\n",
      " 'NAME_EDUCATION_TYPE_Higher education' 'APARTMENTS_AVG' 'CODE_GENDER_F'\n",
      " 'BASEMENTAREA_AVG' 'NONLIVINGAREA_AVG' 'LIVINGAREA_MODE'\n",
      " 'OBS_30_CNT_SOCIAL_CIRCLE']\n"
     ]
    }
   ],
   "source": [
    "# TODO LOOK AT GBM CODE - SAVING BEST MODEL\n",
    "\n",
    "def get_top_gbm_features(path, num_feats):\n",
    "    # get the names of the top n features for gbm \n",
    "    gbm_fi = load_pickle(path)\n",
    "    top_feats =  gbm_fi.sort_values(by=[\"importance\"], ascending=False)[:num_feats]['feature'].values\n",
    "    return top_feats\n",
    "    \n",
    "top_gbm_feats = get_top_gbm_features(\"../misc/gbm_simple_feature_importances.pickle\", 30)\n",
    "print(top_gbm_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1', 'DAYS_BIRTH',\n",
      "       'DAYS_EMPLOYED', 'REGION_RATING_CLIENT_W_CITY', 'REGION_RATING_CLIENT',\n",
      "       'NAME_INCOME_TYPE_Working', 'NAME_EDUCATION_TYPE_Higher education',\n",
      "       'DAYS_LAST_PHONE_CHANGE', 'CODE_GENDER_M', 'CODE_GENDER_F',\n",
      "       'DAYS_ID_PUBLISH', 'REG_CITY_NOT_WORK_CITY',\n",
      "       'NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
      "       'NAME_INCOME_TYPE_Pensioner', 'ORGANIZATION_TYPE_XNA',\n",
      "       'DAYS_EMPLOYED_ANOM', 'FLAG_EMP_PHONE', 'REG_CITY_NOT_LIVE_CITY',\n",
      "       'FLAG_DOCUMENT_3', 'FLOORSMAX_AVG', 'FLOORSMAX_MEDI', 'FLOORSMAX_MODE',\n",
      "       'OCCUPATION_TYPE_Laborers', 'EMERGENCYSTATE_MODE_No',\n",
      "       'DAYS_REGISTRATION', 'HOUSETYPE_MODE_block of flats', 'AMT_GOODS_PRICE',\n",
      "       'OWN_CAR_AGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_correlations(df, feature,):\n",
    "    correlations = df.corr()[feature]\n",
    "    correlations = abs(correlations).sort_values(ascending=False)  # sort by correlation value (regardless if it's positive or negative)\n",
    "    correlations = correlations.dropna()  # drop nans\n",
    "    if 'TARGET' in correlations:\n",
    "        correlations = correlations.drop(labels=[feature, 'TARGET'])  # remove corr to itself and target\n",
    "    return correlations\n",
    "\n",
    "def get_top_correlations(corrs, num_feats):\n",
    "    return corrs[:num_feats].keys()\n",
    "\n",
    "\n",
    "# pcc_corrs = get_sorted_correlations(train_data, 'TARGET')\n",
    "# save_pickle('../misc/target_corr_sorted_pcc.pickle', pcc_corrs)\n",
    "pcc_corrs = load_pickle('../misc/target_corr_sorted_pcc.pickle')\n",
    "# print(pcc_corrs)\n",
    "\n",
    "# pcc_corrs = load_pickle('../misc/target_corr_sorted_pcc.pickle')\n",
    "pcc_corrs = get_top_correlations(pcc_corrs, 30)\n",
    "print(pcc_corrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top features agreed on by both pearsons correlation and lgbm\n",
    "top_corr_feats = set(top_gbm_feats).intersection(set(pcc_corrs))\n",
    "top_corr_feats = list(top_corr_feats)\n",
    "len(top_corr_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.280846\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.367992\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.328725\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.274561\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282745\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.400406\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.351512\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575244\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.331611\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.279522\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.344791\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.403788\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.299431\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369906\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.325719\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.277522\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.291495\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.399790\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.354894\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575718\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.332830\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282784\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.345803\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402360\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.453980\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402015\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.361190\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.392320\n",
      "         Iterations 2\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.456470\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.424545\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.602142\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.405751\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.364074\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.486982\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.463278\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.412782\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.317909\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.328361\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.430651\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.384920\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591520\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.369881\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.321615\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.380188\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.426428\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.300641\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.281309\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.394565\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.345108\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575278\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.327777\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.276889\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.335216\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.399933\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.362019\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.404099\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.364711\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577343\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.341391\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.284148\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.417672\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.404817\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.480795\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.453243\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.609649\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.438642\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.400637\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.516431\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.485505\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.469221\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.596921\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.390798\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.349990\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.518067\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.451352\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575244\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.590103\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.575504\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592018\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611521\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.388007\n",
      "         Iterations 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.329591\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.455509\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.439002\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.309401\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.343963\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.406006\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.541346\n",
      "         Iterations 1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.449322\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.403788\n",
      "         Iterations 7\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.637190\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumika\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:508: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_poly_feats(df, poly_cols, degree):\n",
    "    # cols to create poly feature from\n",
    "    selected_cols = df[poly_cols]\n",
    "    # create polynomial features  to the given degree\n",
    "    poly_transformer = PolynomialFeatures(degree=degree)\n",
    "    poly_transformer.fit(selected_cols)\n",
    "    poly_feats = poly_transformer.transform(selected_cols)\n",
    "    \n",
    "    # get the column names of the polynomial features\n",
    "    col_names = poly_transformer.get_feature_names(poly_cols)\n",
    "    poly_feats_df = pd.DataFrame(poly_feats, columns=col_names)\n",
    "    poly_feats_df= poly_feats_df.drop(columns=['1']) #drop the constant column\n",
    "    return poly_feats_df\n",
    "\n",
    "def calc_poly_aic_score(feats_df, n=10):\n",
    "    aic_scores = []\n",
    "    # calculate the aic score for each of the poly features\n",
    "    for col in feats_df:\n",
    "        aic_scores.append(imputed_col_aic(feats_df, col))\n",
    "    \n",
    "    aic_df = pd.DataFrame({'col':feats_df.columns, 'aic':aic_scores}) \n",
    "    best_n_cols = get_top_aic_cols(aic_df, n)\n",
    "    return aic_df, best_n_cols \n",
    "\n",
    "def get_top_aic_cols(df, n=10):\n",
    "    # return the n features with the lowest aic score\n",
    "    return df.sort_values(by='aic')[:n]['col'].values\n",
    "\n",
    "# get the polynomial features for the training data\n",
    "train_poly_df = create_poly_feats(train_data, top_corr_feats, 2)\n",
    "train_poly_df['TARGET'] = train_data['TARGET'] # add Target col to train\n",
    "\n",
    "# get the polynomial features for the testing\n",
    "test_poly_df = create_poly_feats(test_data, top_corr_feats, 2)\n",
    "\n",
    "# get the aic scores for each poly feature and the best features \n",
    "poly_aic_df, lowest_aic_cols = calc_poly_aic_score(train_poly_df, n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 232) (48744, 231)\n",
      "(307511, 252) (48744, 251)\n",
      "{'TARGET'}\n"
     ]
    }
   ],
   "source": [
    "def merge_poly_feats(train_df, test_df, poly_train, poly_test):\n",
    "    # pass in original train and test dataframes and the dataframes for the poly features\n",
    "    \n",
    "    # Merge polynomial features into training dataframe\n",
    "    print(train_df.shape, test_df.shape)\n",
    "    poly_train['SK_ID_CURR'] = train_df['SK_ID_CURR']\n",
    "    poly_train = train_data.merge(poly_train, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "    #  Merge polynomial features into testing dataframe\n",
    "    poly_test['SK_ID_CURR'] = test_df['SK_ID_CURR']\n",
    "    poly_test = test_data.merge(poly_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "    print(poly_train.shape, poly_test.shape)\n",
    "    return poly_train, poly_test\n",
    "\n",
    "train_poly = train_poly_df[lowest_aic_cols] #top poly feature df\n",
    "test_poly = test_poly_df[lowest_aic_cols] #top poly feature df\n",
    "\n",
    "train_poly, test_poly = merge_poly_feats(train_data, test_data, train_poly, test_poly)\n",
    "\n",
    "# 'TARGET' should be the only extra feature in train_poly\n",
    "print(set(train_poly.columns).difference(set(test_poly.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNED:\n",
      "Training Features shape:  (307511, 252)\n",
      "Testing Features shape:  (48744, 251)\n",
      "Training Data Shape:  (307511, 250)\n",
      "Testing Data Shape:  (48744, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumika\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype bool, uint8, int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALISED:\n",
      "Training data shape:  (307511, 250)\n",
      "Testing data shape:  (48744, 250)\n",
      "Training Data Shape:  (307511, 250)\n",
      "Testing Data Shape:  (48744, 250)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.75423\tvalid's binary_logloss: 0.562766\ttrain's auc: 0.79965\ttrain's binary_logloss: 0.552252\n",
      "Early stopping, best iteration is:\n",
      "[249]\tvalid's auc: 0.754458\tvalid's binary_logloss: 0.557706\ttrain's auc: 0.808133\ttrain's binary_logloss: 0.544999\n",
      "File saved at  ../models/lgbm_poly_feats.pickle\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.757827\tvalid's binary_logloss: 0.563112\ttrain's auc: 0.799626\ttrain's binary_logloss: 0.552356\n",
      "Early stopping, best iteration is:\n",
      "[241]\tvalid's auc: 0.758206\tvalid's binary_logloss: 0.558844\ttrain's auc: 0.806536\ttrain's binary_logloss: 0.546298\n",
      "File saved at  ../models/lgbm_poly_feats.pickle\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.763184\tvalid's binary_logloss: 0.56375\ttrain's auc: 0.798469\ttrain's binary_logloss: 0.553498\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid's auc: 0.763506\tvalid's binary_logloss: 0.557541\ttrain's auc: 0.809135\ttrain's binary_logloss: 0.544527\n",
      "File saved at  ../models/lgbm_poly_feats.pickle\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.758434\tvalid's binary_logloss: 0.561854\ttrain's auc: 0.800111\ttrain's binary_logloss: 0.551965\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid's auc: 0.758614\tvalid's binary_logloss: 0.557657\ttrain's auc: 0.807071\ttrain's binary_logloss: 0.545956\n",
      "File saved at  ../models/lgbm_poly_feats.pickle\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.757704\tvalid's binary_logloss: 0.564392\ttrain's auc: 0.799405\ttrain's binary_logloss: 0.552519\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid's auc: 0.758395\tvalid's binary_logloss: 0.55569\ttrain's auc: 0.81394\ttrain's binary_logloss: 0.540163\n",
      "File saved at  ../models/lgbm_poly_feats.pickle\n",
      "      fold     train     valid\n",
      "0        0  0.808133  0.754458\n",
      "1        1  0.806536  0.758206\n",
      "2        2  0.809135  0.763506\n",
      "3        3  0.807071  0.758614\n",
      "4        4  0.813940  0.758395\n",
      "5  overall  0.808963  0.758622\n",
      "Predictions saved to:  ../test_predictions/lgbm_poly_feats.csv\n"
     ]
    }
   ],
   "source": [
    "# run gbm with the added poly feature this time\n",
    "\n",
    "# gbm run for features\n",
    "train_X, test_X , train_Y= align_data(train_poly, test_poly)\n",
    "train_X = train_X.drop(columns=['TARGET'])\n",
    "train_X = train_X.drop(columns=['SK_ID_CURR'])\n",
    "test_X = test_X.drop(columns=['SK_ID_CURR'])\n",
    "print('Training Data Shape: ', train_X.shape); print('Testing Data Shape: ', test_X.shape)\n",
    "feature_names = train_X.columns\n",
    "train_X, test_X = normalise(train_X, test_X)\n",
    "\n",
    "# test_Y, feature_importances, score_metrics = gbm_basic(train_X, train_Y, test_X, feature_names, \n",
    "#                                                        model_save_path=\"../models/lgbm_poly_feats.pickle\" ,n_folds=5)\n",
    "\n",
    "# print(score_metrics)\n",
    "# create_and_save_submission(test_imputed, test_Y, save_path='../test_predictions/lgbm_poly_feats.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNED:\n",
      "Training Features shape:  (307511, 252)\n",
      "Testing Features shape:  (48744, 251)\n",
      "Training Data Shape:  (307511, 250)\n",
      "Testing Data Shape:  (48744, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumika\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype bool, uint8, int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALISED:\n",
      "Training data shape:  (307511, 250)\n",
      "Testing data shape:  (48744, 250)\n"
     ]
    }
   ],
   "source": [
    "# train_poly = mean_impute(train_poly)\n",
    "# test_poly = mean_impute(test_poly)\n",
    "train_X, test_X , train_Y= align_data(train_poly, test_poly)\n",
    "train_X = train_X.drop(columns=['TARGET'])\n",
    "train_X = train_X.drop(columns=['SK_ID_CURR'])\n",
    "test_X = test_X.drop(columns=['SK_ID_CURR'])\n",
    "print('Training Data Shape: ', train_X.shape); print('Testing Data Shape: ', test_X.shape)\n",
    "feature_names = train_X.columns\n",
    "train_X, test_X = normalise(train_X, test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNED:\n",
      "Training Features shape:  (307511, 252)\n",
      "Testing Features shape:  (48744, 251)\n",
      "Training Data Shape:  (307511, 250)\n",
      "Testing Data Shape:  (48744, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumika\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype bool, uint8, int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALISED:\n",
      "Training data shape:  (307511, 250)\n",
      "Testing data shape:  (48744, 250)\n",
      "File saved at  ../models/log_reg_baseline_poly_feats.pickle\n",
      "Log reg baseline model saved to:  ../models/log_reg_baseline_poly_feats.pickle\n",
      "Predictions saved to:  ../test_predictions/test.csv\n"
     ]
    }
   ],
   "source": [
    "def cross_val_roc_curve(train_X, train_Y, classifier):\n",
    "    # From https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "\n",
    "    # ROC AUC with stratified cross validation\n",
    "    X = train_X\n",
    "    y = train_Y\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=6, shuffle=True)\n",
    "    tprs = []  # true positive rate scores\n",
    "    aucs = []  # area under curve scores\n",
    "    mean_fpr = np.linspace(0, 1, 100)  # mean false positive rates\n",
    "\n",
    "    i = 0\n",
    "    # train and test for each fold\n",
    "    for train_sample, test_sample in cv.split(X, y):\n",
    "        probas_ = classifier.fit(X[train_sample], y[train_sample]).predict_proba(X[test_sample])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_sample], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        print(\"Run {} AUC socre: {}\".format(i, roc_auc))\n",
    "\n",
    "        '''Everything below this point is just for the plot'''\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    # plot roc curve for fold\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Chance', alpha=.8)\n",
    "\n",
    "    # caluclate and plot mean auc\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    # plot standard deviation area\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    # add labels to plot\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    print(\"Avg ROC AUC score: {}\".format(np.mean(aucs)))\n",
    "\n",
    "model = load_pickle(\"../models/log_reg_opt.pickle\")\n",
    "# train_X, test_X, feature_names = normalise_and_impute(train_data, test_data, impute_strategy='mean')\n",
    "cross_val_roc_curve(train_X, train_Y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  22 out of  30 | elapsed:  3.9min remaining:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done  30 out of  30 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1.668088018810296\n",
      "File saved at  ../models/log_reg_tuned_poly_feats.pickle\n",
      "Log reg baseline model saved to:  ../models/log_reg_tuned_poly_feats.pickle\n",
      "Predictions saved to:  ../test_predictions/log_reg_tuned_poly_feats.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def random_search_log_reg(train, train_Y, test, save_path):\n",
    "    '''\n",
    "    Mean imputation\n",
    "    Best C:  1.668088018810296\n",
    "    test acc -> 0.74041\n",
    "    '''\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Search parameters and search space\n",
    "    C = uniform(loc=0, scale=4)\n",
    "    hyperparameters = dict(C=C,)\n",
    "\n",
    "    # Create randomized search 5-fold cross validation and 100 iterations\n",
    "    clf = RandomizedSearchCV(model, hyperparameters, random_state=1, n_iter=10, cv=3, verbose=3, n_jobs=10) # will take a while to run\n",
    "    # Fit randomized search\n",
    "    best_model = clf.fit(train, train_Y)\n",
    "\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "    predictions = best_model.predict_proba(test)[:, 1]\n",
    "\n",
    "    # Save model\n",
    "    save_pickle(save_path, best_model)  # save model\n",
    "    print(\"Log reg baseline model saved to: \", save_path)\n",
    "\n",
    "    return model, predictions\n",
    "\n",
    "model, preds = random_search_log_reg(train_X, train_Y, test_X, save_path=\"../models/log_reg_tuned_poly_feats.pickle\")\n",
    "create_and_save_submission(test_data, preds, save_path='../test_predictions/log_reg_tuned_poly_feats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
