{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis notebook contains code to do knn imputation (with a tuned neighbour value using random search) on the pre_processed\\ndata.\\n\\nTHIS TOOK A VERY LONG TIME TO RUN AND RESULTS WEREN'T GREAT SO NOT WORTH RUNNING AGAIN\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook contains code to do knn imputation (with a tuned neighbour value using random search) on the pre_processed\n",
    "data.\n",
    "\n",
    "THIS TOOK A VERY LONG TIME TO RUN AND RESULTS WEREN'T GREAT SO NOT WORTH RUNNING AGAIN\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_app_training_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded columns ['NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
      "AFTER ONE HOT ENCODING\n",
      "Training Features shape:  (307511, 243)\n",
      "Testing Features shape:  (48744, 239)\n",
      "AFTER ALIGNMENT:\n",
      "Training Features shape:  (307511, 240)\n",
      "Testing Features shape:  (48744, 239)\n",
      "Test data contain 9274 anomalies out of 48744 rows\n",
      "Removing columns with 0.68 proportion of missing values\n",
      "AFTER REMOVING MISSING COLS (and aligning):\n",
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 240)\n"
     ]
    }
   ],
   "source": [
    "# pre-processing (No imputation or normalisation is applied)\n",
    "train_data, test_data = encode_binary_cols(train_data, test_data)\n",
    "train_data, test_data = one_hot_encode(train_data, test_data)\n",
    "train_data, test_data = align_data(train_data, test_data)\n",
    "train_Y = get_train_labels(train_data)\n",
    "train_data, test_data = remove_days_employed_anomaly(train_data, test_data)\n",
    "train_data, test_data = remove_missing_cols(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_correlations(app_train, feature, n=15):\n",
    "    '''\n",
    "    Return a dataframe containing the top n features ranked by their pearson's correlations \n",
    "    (sorted by their absolute pcc score) \n",
    "    '''\n",
    "    correlations = app_train.corr()[feature]\n",
    "    correlations = abs(correlations).sort_values().tail(\n",
    "        n)  # sort by correlation value (regardless if it's positive or negative)\n",
    "    correlations = correlations.dropna()  # drop nans\n",
    "    if 'TARGET' in correlations:\n",
    "        correlations = correlations.drop(labels=[feature, 'TARGET'])  # remove corr to itself and target\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_correlations(app_train):\n",
    "    \"\"\"\n",
    "    For each feature will save a dataframe storing all correlations with the feature and every other feature\n",
    "    \"\"\"\n",
    "    path = '../models/correlations/'\n",
    "    for feature in train_data.columns[:]:\n",
    "        correlations = app_train.corr()[feature]\n",
    "        if '/' in feature or ':' in feature:\n",
    "            # replace / and : with '' -> as they are invalid characters for filename \n",
    "            feature = feature.replace('/','')\n",
    "            feature = feature.replace(':','')\n",
    "        save_pickle(path+feature+'.pickle', correlations)\n",
    "    print(\"ALL CORRELATIONS SAVED\")\n",
    "\n",
    "def top_correlations_from_all(feature:str, n):\n",
    "    \"\"\"\n",
    "    Loads a previously saved dataframe containing feature corrleations and returns the top n correlated features\n",
    "    (excludes target)\n",
    "    \"\"\"\n",
    "    path = '../models/correlations/'\n",
    "    filename = feature\n",
    "    if '/' in filename or ':' in filename:\n",
    "            # replace / and : with '' -> as they are invalid characters for filename \n",
    "            filename = filename.replace('/','')\n",
    "            filename = filename.replace(':','')\n",
    "            \n",
    "    correlations = load_pickle(path+feature+'.pickle')\n",
    "    correlations = abs(correlations).sort_values().tail(n)  # sort by correlation value (regardless if it's positive or negative)\n",
    "    correlations = correlations.dropna()  # drop nans\n",
    "    if 'TARGET' in correlations:\n",
    "        correlations = correlations.drop(labels=[feature, 'TARGET'])  # remove corr to itself and target\n",
    "    return correlations\n",
    "    \n",
    "# save_all_correlations(train_data)\n",
    "# train_data.columns[181:][0].replace('/', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_corr_knn_imputation(feature, train, test, train_filled, test_filled,n_jobs):\n",
    "    \"\"\"\n",
    "    Applies knn imputation for a single column. Will use random search to try find the best neighbour\n",
    "    value (range 10-200). Returns a dataframes for the train and test with imputed column\n",
    "    \"\"\"\n",
    "    top_corrs = top_correlations_from_all(feature, 15).keys()\n",
    "    \n",
    "    # create training data from columns with values\n",
    "    not_null_mask = train[feature].notna()  # true if not nan = mask to get the values to train on\n",
    "    knn_train_x = train_filled[not_null_mask][top_corrs] # get all top corr cols (minus target) which have a feature value\n",
    "    knn_train_y = train_filled[not_null_mask][[feature]]\n",
    "    \n",
    "    # get the rows which require to be imputed for the features\n",
    "    knn_test_x = train_filled[train[feature].isna()][top_corrs]  # only null values\n",
    "    actual_test_x = test_filled[test[feature].isna()][top_corrs]  # only null values\n",
    "    \n",
    "    # train and predict\n",
    "    filename = feature.replace(':', '')\n",
    "    fileaname = feature.replace('/', '')\n",
    "    try:\n",
    "        neigh = load_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Training col:\", feature)\n",
    "    #     neigh = KNeighborsRegressor(n_neighbors=75)\n",
    "        model = KNeighborsRegressor()\n",
    "        n_neighbors = range(10,200)\n",
    "        hyperparameters = dict(n_neighbors=n_neighbors)\n",
    "        neigh = RandomizedSearchCV(model, hyperparameters, n_iter=7, cv=3, verbose=3, n_jobs=n_jobs)\n",
    "        neigh.fit(knn_train_x, knn_train_y)\n",
    "        save_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\", neigh)\n",
    "        \n",
    "    print('Best n_neigbors:', neigh.best_estimator_.get_params()['n_neighbors'])\n",
    "    knn_test_y = neigh.predict(knn_test_x)\n",
    "    # get the column to update nulls\n",
    "    train_imputed_col = train[[feature]]\n",
    "    train_imputed_col[train_imputed_col[feature].isna()] = knn_test_y\n",
    "    \n",
    "    # the train data has more unfilled cols than test, so sometimes imputation is not required\n",
    "    try:\n",
    "        actual_test_y = neigh.predict(actual_test_x)\n",
    "        # get the column to update nulls for app_test\n",
    "        test_imputed_col = test[[feature]]\n",
    "        test_imputed_col[test_imputed_col[feature].isna()] = actual_test_y\n",
    "    except ValueError:\n",
    "        test_imputed_col = test[[feature]]\n",
    "    \n",
    "    return train_imputed_col, test_imputed_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(app_train, app_test, n_jobs):\n",
    "    \"\"\"\n",
    "    Applies knn imputation for all columns with missing values. KNN requires all other columns to be not null (so a \n",
    "    mean imputation is used to fill those columns in)\n",
    "    \"\"\"\n",
    "    count = 1\n",
    "    train_nan_cols = app_train.loc[:,app_train.isnull().any()]\n",
    "    test_nan_cols = app_test.loc[:,app_test.isnull().any()]\n",
    "    \n",
    "    train_full_impute = app_train.copy()\n",
    "    train_full_impute = train_full_impute.fillna(train_full_impute.mean())\n",
    "    test_full_impute = app_test.copy()\n",
    "    test_full_impute  = test_full_impute.fillna(train_full_impute.mean())\n",
    "\n",
    "    print(list(train_nan_cols))\n",
    "    for col in train_nan_cols: \n",
    "        imputed_train, imputed_test = top_corr_knn_imputation(col, app_train, app_test, train_full_impute, test_full_impute, n_jobs=n_jobs)\n",
    "        app_train[col] = imputed_train\n",
    "        app_test[col] = imputed_test\n",
    "        print(\"{} out of {} done - col: {}\".format(count, test_nan_cols.shape[1], col))\n",
    "        count += 1\n",
    "            \n",
    "    return app_train, app_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_EMPLOYED', 'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/correlations/AMT_ANNUITY.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e63932920bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                        \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../models/knn_imputation_top_corr_log_reg1.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                        \u001b[0mpredications_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../test_predictions/knn_imputation_top_corr_log_reg1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                        n_jobs=10) \n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-e63932920bc8>\u001b[0m in \u001b[0;36mlog_reg_knn_imputation\u001b[0;34m(app_train, app_test, model_save_path, predications_save_path, n_jobs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mImpute\u001b[0m \u001b[0mon\u001b[0m \u001b[0mall\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mvia\u001b[0m \u001b[0mknn\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mcreate\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0musing\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mreg\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0many\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0mon\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpute_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     save_pickle(\"../models/knn_impute_train_df1.pickle\", train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     save_pickle(\"../models/knn_impute_test_df1.pickle\", test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c1b17d9b6c01>\u001b[0m in \u001b[0;36mimpute_knn\u001b[0;34m(app_train, app_test, n_jobs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nan_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_nan_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimputed_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputed_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_corr_knn_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_full_impute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_full_impute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mapp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputed_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mapp_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputed_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d85320cf55dd>\u001b[0m in \u001b[0;36mtop_corr_knn_imputation\u001b[0;34m(feature, train, test, train_filled, test_filled, n_jobs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataframes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mimputed\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtop_corrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_correlations_from_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# create training data from columns with values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5f1848699299>\u001b[0m in \u001b[0;36mtop_correlations_from_all\u001b[0;34m(feature, n)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sort by correlation value (regardless if it's positive or negative)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# drop nans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Scoala/Data Mining/data-mining/src/utils.py\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munpickled\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File loaded: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/correlations/AMT_ANNUITY.pickle'"
     ]
    }
   ],
   "source": [
    "def log_reg_knn_imputation( app_train, app_test, model_save_path, predications_save_path, n_jobs):\n",
    "    \"\"\"\n",
    "    Impute on all columns via knn then create predictions using log reg without any tuning on C\n",
    "    \"\"\"\n",
    "    train, test = impute_knn(app_train, app_test, n_jobs=n_jobs)\n",
    "#     save_pickle(\"../models/knn_impute_train_df1.pickle\", train)\n",
    "#     save_pickle(\"../models/knn_impute_test_df1.pickle\", test)\n",
    "    train = load_pickle(\"../models/knn_impute_train_df.pickle\")\n",
    "    test = load_pickle(\"../models/knn_impute_test_df.pickle\")\n",
    "    train_Y = train['TARGET']\n",
    "\n",
    "    train = train.drop(columns=train.loc[:,train.isnull().any()].columns)  #\n",
    "    trian, test, train_Y = align_data(train,test)\n",
    "    # Drop the target and sk_id_curr from the training data\n",
    "    if 'TARGET' in train:\n",
    "        train = train.drop(columns=['TARGET'])\n",
    "    train = train.drop(columns=['SK_ID_CURR'])  #\n",
    "    test = test.drop(columns=['SK_ID_CURR'])  #\n",
    "    \n",
    "    model, preds = baseline_log_reg(train, train_Y, test, save_path=model_save_path)\n",
    "    create_and_save_submission(test_data, preds, save_path=predications_save_path)\n",
    "\n",
    "    \n",
    "log_reg_knn_imputation(train_data.copy(), test_data.copy(),\n",
    "                       model_save_path=\"../models/knn_imputation_top_corr_log_reg1.pickle\",\n",
    "                       predications_save_path='../test_predictions/knn_imputation_top_corr_log_reg1.csv',\n",
    "                       n_jobs=10) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
