{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook focuses on pre-processing and creating a baseline model to improve upon\\nPre-processing includes:\\n - Dummy encoding\\n - Imputation\\n - Removal of columns with high missing variables\\n\\nLogistic regression model applied (no optimisation)\\nAccuracy -> ___%\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook focuses on pre-processing and creating a baseline model to improve upon\n",
    "Pre-processing includes:\n",
    " - Dummy encoding\n",
    " - Imputation \n",
    "     - mean, median, mode and KNN -> Mean used in end\n",
    " - (Removal of columns with high missing variables) -> found does not work well\n",
    " \n",
    "Logistic regression model  (no optimisation)\n",
    "Logistic regression model  (with basic random search optimisation)\n",
    "\n",
    "Best ROC AUC on test set -> 73.846%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, data):\n",
    "    # pickles the tokens dict\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"File saved at \", path)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    # loads the tokens dict from directory\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    print(\"File loaded: \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "def load_training_data():\n",
    "    app_train = pd.read_csv('../input/application_train.csv')\n",
    "    print('Training data shape: ', app_train.shape)\n",
    "    return app_train\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    # Testing data features\n",
    "    app_test = pd.read_csv('../input/application_test.csv')\n",
    "    print('Testing data shape: ', app_test.shape)\n",
    "    return app_test\n",
    "\n",
    "train_data = load_training_data()\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_binary_cols(app_train, app_test):\n",
    "    # Create a label encoder object\n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "    encoded_cols = []\n",
    "    # Iterate through the columns\n",
    "    for col in app_train:\n",
    "        if app_train[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories (a nan will count as a category)\n",
    "            if len(list(app_train[col].unique())) <= 2:\n",
    "                # Train on the training data\n",
    "                le.fit(app_train[col])\n",
    "                # Transform both training and testing data\n",
    "                app_train[col] = le.transform(app_train[col])\n",
    "                app_test[col] = le.transform(app_test[col])\n",
    "                encoded_cols.append(col)\n",
    "\n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "    return app_train, app_test\n",
    "                \n",
    "train_data, test_data = encode_binary_cols(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE HOT ENCODED\n",
      "Training Features shape:  (307511, 243)\n",
      "Testing Features shape:  (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(app_train, app_test):\n",
    "    # one-hot encoding of categorical variables\n",
    "    # Dummy encoding will not create a column for nans\n",
    "    app_train = pd.get_dummies(app_train)\n",
    "    app_test = pd.get_dummies(app_test)\n",
    "\n",
    "    print(\"ONE HOT ENCODED\")\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    return app_train, app_test\n",
    "\n",
    "train_data, test_data = one_hot_encode(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNED:\n",
      "Training Features shape:  (307511, 240)\n",
      "Testing Features shape:  (48744, 239)\n"
     ]
    }
   ],
   "source": [
    "def align_data(app_train, app_test):\n",
    "    # ALIGN TEST AND TRAIN DATAFRAMES SO COLUMNS MATCH\n",
    "    train_labels = app_train['TARGET']\n",
    "\n",
    "    # Align the training and testing data, keep only columns present in both dataframes\n",
    "    app_train, app_test = app_train.align(app_test, join='inner', axis=1)\n",
    "\n",
    "    # Add the target back in\n",
    "    app_train['TARGET'] = train_labels\n",
    "\n",
    "    print(\"ALIGNED:\")\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    return app_train, app_test, train_labels\n",
    "\n",
    "train_data, test_data, train_Y = align_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 9274 anomalies in the test data out of 48744 entries\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_days_employed_anomaly(app_train, app_test):\n",
    "    # DEALING WITH ANOMALOUS DATA IN 'DAYS_EMPLOYED' COL\n",
    "\n",
    "    # Create an anomalous flag column\n",
    "    app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "    # Replace the anomalous values with nan\n",
    "    app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "    app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram');\n",
    "    plt.xlabel('Days Employment');\n",
    "\n",
    "    app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "    app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "    print('There were %d anomalies in the test data out of %d entries' % (\n",
    "        app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n",
    "    return app_train, app_test\n",
    "\n",
    "train_data, test_data = remove_days_employed_anomaly(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 240)\n",
      "Training Features shape:  (307511, 232)\n",
      "Testing Features shape:  (48744, 231)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best Penalty: l2\n",
    "Best C: 0.7450408455106836\n",
    "Score - 0.73806\n",
    "Approx 20 mins train time\n",
    "'''\n",
    "def remove_missing_cols(app_train, app_test, thr=0.68):\n",
    "\n",
    "    app_train = app_train.loc[:, app_train.isnull().mean() < thr] #remove all columns with more than x% missing values \n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "\n",
    "    # ALIGN TEST AND TRAIN DATAFRAMES SO COLUMNS MATCH\n",
    "    train_labels = app_train['TARGET']\n",
    "    # Align the training and testing data, keep only columns present in both dataframes\n",
    "    app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "    # Add the target back in\n",
    "    app_train['TARGET'] = train_labels\n",
    "\n",
    "    print('Training Features shape: ', app_train.shape)\n",
    "    print('Testing Features shape: ', app_test.shape)\n",
    "    return app_train, app_test\n",
    "    \n",
    "# train_data, test_data = remove_missing_cols(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPUTED AND NORMALISED\n",
      "Training data shape:  (307511, 230)\n",
      "Testing data shape:  (48744, 230)\n"
     ]
    }
   ],
   "source": [
    "def normalise_and_impute(app_train, app_test, impute_strategy='mean'):\n",
    "    # Drop the target from the training data\n",
    "    if 'TARGET' in app_train:\n",
    "        train = app_train.drop(columns=['TARGET'])\n",
    "    else:\n",
    "        train = app_train.copy()\n",
    "    train = train.drop(columns=['SK_ID_CURR'])  #\n",
    "\n",
    "    test = app_test.copy()\n",
    "    test = test.drop(columns=['SK_ID_CURR'])  #\n",
    "\n",
    "    # Feature names\n",
    "    features = list(train.columns)\n",
    "\n",
    "    # Median imputation of missing values\n",
    "    imputer = Imputer(strategy=impute_strategy)\n",
    "    # Fit on the training data\n",
    "    imputer.fit(train)\n",
    "    # Transform both training and testing data\n",
    "    train = imputer.transform(train)\n",
    "    test = imputer.transform(test)  ### test was app_test\n",
    "\n",
    "    # Normalise\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Scale each feature to 0-1\n",
    "    scaler.fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "    print(\"IMPUTED AND NORMALISED\")\n",
    "    print('Training data shape: ', train.shape)\n",
    "    print('Testing data shape: ', test.shape)\n",
    "    return train, test, features\n",
    "\n",
    "# train_X and test_X are np arrays\n",
    "# train_X, test_X, feature_names = normalise_and_impute(train_data, test_data, impute_strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_submission(app_test, predictions, save_path):\n",
    "    # Submission dataframe\n",
    "    submit = app_test[['SK_ID_CURR']]\n",
    "    submit['TARGET'] = predictions\n",
    "\n",
    "    # Save the submission to a csv file\n",
    "    submit.to_csv(save_path, index=False)\n",
    "    print(\"Predictions saved to: \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at  ../models/test.pickle\n",
      "Log reg baseline model saved to:  ../models/test.pickle\n",
      "Predictions saved to:  ../test_predictions/test.csv\n"
     ]
    }
   ],
   "source": [
    "def baseline_log_reg(train, train_Y, test, save_path):\n",
    "    # Make the model with the specified regularization parameter\n",
    "    log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "    # Train on the training data\n",
    "    log_reg.fit(train, train_Y)\n",
    "\n",
    "    # Make predictions - only require 2nd columns (representing the probability that the target is 1)\n",
    "    log_reg_pred = log_reg.predict_proba(test)[:, 1]\n",
    "\n",
    "    # Save model\n",
    "    save_pickle(save_path, log_reg)  # save model\n",
    "    print(\"Log reg baseline model saved to: \", save_path)\n",
    "    return log_reg, log_reg_pred\n",
    "\n",
    "model, preds = baseline_log_reg(train_X, train_Y, test_X, save_path=\"../models/test.pickle\")\n",
    "create_and_save_submission(test_data, preds, save_path='../test_predictions/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  22 out of  30 | elapsed: 13.9min remaining:  5.1min\n",
      "[Parallel(n_jobs=10)]: Done  30 out of  30 | elapsed: 16.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 0.7450408455106836\n",
      "File saved at  ../models/log_reg_new.pickle\n",
      "Log reg baseline model saved to:  ../models/log_reg_drop_rs_tuned.pickle\n",
      "Predictions saved to:  ../test_predictions/log_reg_drop_rs_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "def random_search_log_reg(train, train_Y, test, save_path):\n",
    "    '''\n",
    "    Mean imputation\n",
    "    Best Penalty: l2\n",
    "    Best C: 1.668088018810296\n",
    "    test acc -> 0.73846\n",
    "    '''\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Search parameters and search space\n",
    "    penalty = ['l1', 'l2']\n",
    "    C = uniform(loc=0, scale=4)\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "    # Create randomized search 5-fold cross validation and 100 iterations\n",
    "    clf = RandomizedSearchCV(model, hyperparameters, random_state=1, n_iter=10, cv=3, verbose=3, n_jobs=10) # will take a while to run\n",
    "    # Fit randomized search\n",
    "    best_model = clf.fit(train, train_Y)\n",
    "\n",
    "    print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "    predictions = best_model.predict_proba(test)[:, 1]\n",
    "\n",
    "    # Save model\n",
    "    save_pickle(save_path, best_model)  # save model\n",
    "    print(\"Log reg baseline model saved to: \", save_path)\n",
    "\n",
    "    return model, predictions\n",
    "\n",
    "# model, preds = random_search_log_reg(train_X, train_Y, test_X, save_path=\"../models/log_reg_drop_rs_tuned.pickle\")\n",
    "# create_and_save_submission(test_data, preds, save_path='../test_predictions/log_reg_drop_rs_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_roc_curve(train_X, train_Y, classifier):\n",
    "    # From https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "\n",
    "    # ROC AUC with stratified cross validation\n",
    "    X = train_X\n",
    "    y = train_Y\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=6, shuffle=True)\n",
    "    tprs = []  # true positive rate scores\n",
    "    aucs = []  # area under curve scores\n",
    "    mean_fpr = np.linspace(0, 1, 100)  # mean false positive rates\n",
    "\n",
    "    i = 0\n",
    "    # train and test for each fold\n",
    "    for train_sample, test_sample in cv.split(X, y):\n",
    "        probas_ = classifier.fit(X[train_sample], y[train_sample]).predict_proba(X[test_sample])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_sample], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        print(\"Run {} AUC socre: {}\".format(i, roc_auc))\n",
    "\n",
    "        '''Everything below this point is just for the plot'''\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    # plot roc curve for fold\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Chance', alpha=.8)\n",
    "\n",
    "    # caluclate and plot mean auc\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    # plot standard deviation area\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    # add labels to plot\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    print(\"Avg ROC AUC score: {}\".format(np.mean(aucs)))\n",
    "\n",
    "# model = load_pickle(\"../models/log_reg_opt.pickle\")\n",
    "# train_X, test_X, feature_names = normalise_and_impute(train_data, test_data, impute_strategy='mean')\n",
    "# cross_val_roc_curve(train_X, train_Y, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_aic_bic(app_train, feature_name: str):\n",
    "    # calculates the aic and bic values between the target and a column\n",
    "    # http://www.differencebetween.net/miscellaneous/difference-between-aic-and-bic/\n",
    "\n",
    "    data = app_train.copy()\n",
    "    data = data[[feature_name, \"TARGET\"]]\n",
    "\n",
    "    # Median imputation of missing values\n",
    "    imputer = Imputer(strategy='mean')\n",
    "    imputer.fit(data)\n",
    "    imputed_data = imputer.transform(data)\n",
    "    data[feature_name] = imputed_data\n",
    "\n",
    "    data[\"intercept\"] = 1.0\n",
    "    logit = sm.Logit(data[\"TARGET\"], data[feature_name])\n",
    "    result = logit.fit()\n",
    "\n",
    "    # result.summary2() # uncomment for full summary\n",
    "    print(\"Selected Feature\", feature_name)\n",
    "    print(\"AIC\", result.aic)\n",
    "    print(\"BIC\", result.bic)\n",
    "\n",
    "    # 168863, 0.274561 - ext3 mean\n",
    "# model = load_pickle(\"../models/log_reg_opt.pickle\")\n",
    "# feature_aic_bic(train_data, \"EXT_SOURCE_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputed_col_aic(data, feature_name):\n",
    "#     data[\"intercept\"] = 1.0\n",
    "    logit = sm.Logit(data[\"TARGET\"], data[feature_name])\n",
    "    result = logit.fit()\n",
    "    print(\"AIC\", result.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_correlations(app_train, feature, n=15):\n",
    "    correlations = app_train.corr()[feature]\n",
    "    correlations = abs(correlations).sort_values().tail(\n",
    "        n)  # sort by correlation value (regardless if it's positive or negative)\n",
    "    correlations = correlations.dropna()  # drop nans\n",
    "    if 'TARGET' in correlations:\n",
    "        correlations = correlations.drop(labels=[feature, 'TARGET'])  # remove corr to itself and target\n",
    "    return correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_correlations(app_train):\n",
    "    path = '../models/correlations/'\n",
    "    for feature in train_data.columns[:]:\n",
    "        correlations = app_train.corr()[feature]\n",
    "        if '/' in feature or ':' in feature:\n",
    "            # replace / and : with '' -> as they are invalid characters for filename \n",
    "            feature = feature.replace('/','')\n",
    "            feature = feature.replace(':','')\n",
    "        save_pickle(path+feature+'.pickle', correlations)\n",
    "    print(\"ALL CORRELATIONS SAVED\")\n",
    "\n",
    "def top_correlations_from_all(feature:str, n):\n",
    "    path = '../models/correlations/'\n",
    "    filename = feature\n",
    "    if '/' in filename or ':' in filename:\n",
    "            # replace / and : with '' -> as they are invalid characters for filename \n",
    "            filename = filename.replace('/','')\n",
    "            filename = filename.replace(':','')\n",
    "            \n",
    "    correlations = load_pickle(path+feature+'.pickle')\n",
    "    correlations = abs(correlations).sort_values().tail(n)  # sort by correlation value (regardless if it's positive or negative)\n",
    "    correlations = correlations.dropna()  # drop nans\n",
    "    if 'TARGET' in correlations:\n",
    "        correlations = correlations.drop(labels=[feature, 'TARGET'])  # remove corr to itself and target\n",
    "    return correlations\n",
    "    \n",
    "# save_all_correlations(train_data)\n",
    "# train_data.columns[181:][0].replace('/', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_corr_knn_imputation(feature, train, test, train_filled, test_filled,n_jobs):\n",
    "    top_corrs = top_correlations_from_all(feature, 15).keys()\n",
    "    \n",
    "    # create training data from columns with values\n",
    "    not_null_mask = train[feature].notna()  # true if not nan = mask to get the values to train on\n",
    "    knn_train_x = train_filled[not_null_mask][top_corrs] # get all top corr cols (minus target) which have a feature value\n",
    "    knn_train_y = train_filled[not_null_mask][[feature]]\n",
    "    \n",
    "    # get the rows which require to be imputed for the features\n",
    "    knn_test_x = train_filled[train[feature].isna()][top_corrs]  # only null values\n",
    "    actual_test_x = test_filled[test[feature].isna()][top_corrs]  # only null values\n",
    "    \n",
    "    # train and predict\n",
    "    filename = feature.replace(':', '')\n",
    "    fileaname = feature.replace('/', '')\n",
    "    try:\n",
    "        neigh = load_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Training col:\", feature)\n",
    "    #     neigh = KNeighborsRegressor(n_neighbors=75)\n",
    "        model = KNeighborsRegressor()\n",
    "        n_neighbors = range(10,200)\n",
    "        hyperparameters = dict(n_neighbors=n_neighbors)\n",
    "        neigh = RandomizedSearchCV(model, hyperparameters, n_iter=7, cv=3, verbose=3, n_jobs=n_jobs)\n",
    "        neigh.fit(knn_train_x, knn_train_y)\n",
    "        save_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\", neigh)\n",
    "        \n",
    "    print('Best n_neigbors:', neigh.best_estimator_.get_params()['n_neighbors'])\n",
    "    knn_test_y = neigh.predict(knn_test_x)\n",
    "    # get the column to update nulls\n",
    "    train_imputed_col = train[[feature]]\n",
    "    train_imputed_col[train_imputed_col[feature].isna()] = knn_test_y\n",
    "    \n",
    "    # the train data has more unfilled cols than test, so something imputation is not required\n",
    "    try:\n",
    "        actual_test_y = neigh.predict(actual_test_x)\n",
    "        # get the column to update nulls for app_test\n",
    "        test_imputed_col = test[[feature]]\n",
    "        test_imputed_col[test_imputed_col[feature].isna()] = actual_test_y\n",
    "    except ValueError:\n",
    "        test_imputed_col = test[[feature]]\n",
    "    \n",
    "    return train_imputed_col, test_imputed_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 62)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(app_train, app_test, n_jobs):\n",
    "    count = 1\n",
    "    train_nan_cols = app_train.loc[:,app_train.isnull().any()]\n",
    "    test_nan_cols = app_test.loc[:,app_test.isnull().any()]\n",
    "    \n",
    "    train_full_impute = app_train.copy()\n",
    "    train_full_impute = train_full_impute.fillna(train_full_impute.mean())\n",
    "    test_full_impute = app_test.copy()\n",
    "    test_full_impute  = test_full_impute.fillna(train_full_impute.mean())\n",
    "\n",
    "    print(list(train_nan_cols))\n",
    "#     for col in train_nan_cols.columns[54:]: # TODO - remove index once done\n",
    "    for col in train_nan_cols: # TODO - remove index once done\n",
    "        imputed_train, imputed_test = top_corr_knn_imputation(col, app_train, app_test, train_full_impute, test_full_impute, n_jobs=n_jobs)\n",
    "        app_train[col] = imputed_train\n",
    "        app_test[col] = imputed_test\n",
    "        print(\"{} out of {} done - col: {}\".format(count, test_nan_cols.shape[1], col))\n",
    "        count += 1\n",
    "            \n",
    "    return app_train, app_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_EMPLOYED', 'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
      "Best n_neigbors: 50\n",
      "1 out of 59 done - col: AMT_ANNUITY\n",
      "Best n_neigbors: 67\n",
      "2 out of 59 done - col: AMT_GOODS_PRICE\n",
      "Best n_neigbors: 93\n",
      "3 out of 59 done - col: DAYS_EMPLOYED\n",
      "Best n_neigbors: 19\n",
      "4 out of 59 done - col: OWN_CAR_AGE\n",
      "Best n_neigbors: 174\n",
      "5 out of 59 done - col: CNT_FAM_MEMBERS\n",
      "Best n_neigbors: 174\n",
      "6 out of 59 done - col: EXT_SOURCE_1\n",
      "Best n_neigbors: 115\n",
      "7 out of 59 done - col: EXT_SOURCE_2\n",
      "Best n_neigbors: 193\n",
      "8 out of 59 done - col: EXT_SOURCE_3\n",
      "Best n_neigbors: 34\n",
      "9 out of 59 done - col: APARTMENTS_AVG\n",
      "Best n_neigbors: 44\n",
      "10 out of 59 done - col: BASEMENTAREA_AVG\n",
      "Best n_neigbors: 35\n",
      "11 out of 59 done - col: YEARS_BEGINEXPLUATATION_AVG\n",
      "Best n_neigbors: 16\n",
      "12 out of 59 done - col: YEARS_BUILD_AVG\n",
      "Best n_neigbors: 26\n",
      "13 out of 59 done - col: COMMONAREA_AVG\n",
      "Best n_neigbors: 24\n",
      "14 out of 59 done - col: ELEVATORS_AVG\n",
      "Best n_neigbors: 16\n",
      "15 out of 59 done - col: ENTRANCES_AVG\n",
      "Best n_neigbors: 46\n",
      "16 out of 59 done - col: FLOORSMAX_AVG\n",
      "Best n_neigbors: 13\n",
      "17 out of 59 done - col: FLOORSMIN_AVG\n",
      "Best n_neigbors: 35\n",
      "18 out of 59 done - col: LANDAREA_AVG\n",
      "Best n_neigbors: 36\n",
      "19 out of 59 done - col: LIVINGAPARTMENTS_AVG\n",
      "Best n_neigbors: 25\n",
      "20 out of 59 done - col: LIVINGAREA_AVG\n",
      "Best n_neigbors: 20\n",
      "21 out of 59 done - col: NONLIVINGAPARTMENTS_AVG\n",
      "Best n_neigbors: 90\n",
      "22 out of 59 done - col: NONLIVINGAREA_AVG\n",
      "Best n_neigbors: 12\n",
      "23 out of 59 done - col: APARTMENTS_MODE\n",
      "Best n_neigbors: 79\n",
      "24 out of 59 done - col: BASEMENTAREA_MODE\n",
      "Best n_neigbors: 19\n",
      "25 out of 59 done - col: YEARS_BEGINEXPLUATATION_MODE\n",
      "Best n_neigbors: 27\n",
      "26 out of 59 done - col: YEARS_BUILD_MODE\n",
      "Best n_neigbors: 20\n",
      "27 out of 59 done - col: COMMONAREA_MODE\n",
      "Best n_neigbors: 18\n",
      "28 out of 59 done - col: ELEVATORS_MODE\n",
      "Best n_neigbors: 18\n",
      "29 out of 59 done - col: ENTRANCES_MODE\n",
      "Best n_neigbors: 57\n",
      "30 out of 59 done - col: FLOORSMAX_MODE\n",
      "Best n_neigbors: 25\n",
      "31 out of 59 done - col: FLOORSMIN_MODE\n",
      "Best n_neigbors: 57\n",
      "32 out of 59 done - col: LANDAREA_MODE\n",
      "Best n_neigbors: 12\n",
      "33 out of 59 done - col: LIVINGAPARTMENTS_MODE\n",
      "Best n_neigbors: 10\n",
      "34 out of 59 done - col: LIVINGAREA_MODE\n",
      "Best n_neigbors: 14\n",
      "35 out of 59 done - col: NONLIVINGAPARTMENTS_MODE\n",
      "Best n_neigbors: 10\n",
      "36 out of 59 done - col: NONLIVINGAREA_MODE\n",
      "Best n_neigbors: 19\n",
      "37 out of 59 done - col: APARTMENTS_MEDI\n",
      "Best n_neigbors: 29\n",
      "38 out of 59 done - col: BASEMENTAREA_MEDI\n",
      "Best n_neigbors: 21\n",
      "39 out of 59 done - col: YEARS_BEGINEXPLUATATION_MEDI\n",
      "Best n_neigbors: 17\n",
      "40 out of 59 done - col: YEARS_BUILD_MEDI\n",
      "Best n_neigbors: 20\n",
      "41 out of 59 done - col: COMMONAREA_MEDI\n",
      "Best n_neigbors: 19\n",
      "42 out of 59 done - col: ELEVATORS_MEDI\n",
      "Best n_neigbors: 36\n",
      "43 out of 59 done - col: ENTRANCES_MEDI\n",
      "Best n_neigbors: 29\n",
      "44 out of 59 done - col: FLOORSMAX_MEDI\n",
      "Best n_neigbors: 10\n",
      "45 out of 59 done - col: FLOORSMIN_MEDI\n",
      "Best n_neigbors: 30\n",
      "46 out of 59 done - col: LANDAREA_MEDI\n",
      "Best n_neigbors: 30\n",
      "47 out of 59 done - col: LIVINGAPARTMENTS_MEDI\n",
      "Best n_neigbors: 31\n",
      "48 out of 59 done - col: LIVINGAREA_MEDI\n",
      "Best n_neigbors: 14\n",
      "49 out of 59 done - col: NONLIVINGAPARTMENTS_MEDI\n",
      "Best n_neigbors: 17\n",
      "50 out of 59 done - col: NONLIVINGAREA_MEDI\n",
      "Best n_neigbors: 52\n",
      "51 out of 59 done - col: TOTALAREA_MODE\n",
      "Best n_neigbors: 87\n",
      "52 out of 59 done - col: OBS_30_CNT_SOCIAL_CIRCLE\n",
      "Best n_neigbors: 21\n",
      "53 out of 59 done - col: DEF_30_CNT_SOCIAL_CIRCLE\n",
      "Best n_neigbors: 47\n",
      "54 out of 59 done - col: OBS_60_CNT_SOCIAL_CIRCLE\n",
      "Best n_neigbors: 11\n",
      "55 out of 59 done - col: DEF_60_CNT_SOCIAL_CIRCLE\n",
      "Best n_neigbors: 31\n",
      "56 out of 59 done - col: DAYS_LAST_PHONE_CHANGE\n",
      "Best n_neigbors: 33\n",
      "57 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_HOUR\n",
      "Best n_neigbors: 11\n",
      "58 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_DAY\n",
      "Best n_neigbors: 24\n",
      "59 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_WEEK\n",
      "Best n_neigbors: 11\n",
      "60 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_MON\n",
      "Best n_neigbors: 31\n",
      "61 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_QRT\n",
      "Best n_neigbors: 113\n",
      "62 out of 59 done - col: AMT_REQ_CREDIT_BUREAU_YEAR\n",
      "File saved at  ../models/knn_impute_train_df1.pickle\n",
      "File saved at  ../models/knn_impute_test_df1.pickle\n",
      "ALIGNED:\n",
      "Training Features shape:  (307511, 241)\n",
      "Testing Features shape:  (48744, 240)\n",
      "File saved at  ../models/log_reg_new.pickle\n",
      "Log reg baseline model saved to:  ../models/knn_imputation_top_corr_log_reg1.pickle\n",
      "Predictions saved to:  ../test_predictions/knn_imputation_top_corr_log_reg1.csv\n"
     ]
    }
   ],
   "source": [
    "def log_reg_knn_imputation( app_train, app_test, model_save_path, predications_save_path, n_jobs):\n",
    "    train, test = impute_knn(app_train, app_test, n_jobs=n_jobs)\n",
    "#     save_pickle(\"../models/knn_impute_train_df1.pickle\", train)\n",
    "#     save_pickle(\"../models/knn_impute_test_df1.pickle\", test)\n",
    "    train = load_pickle(\"../models/knn_impute_train_df.pickle\")\n",
    "    test = load_pickle(\"../models/knn_impute_test_df.pickle\")\n",
    "    train_Y = train['TARGET']\n",
    "\n",
    "    train = train.drop(columns=train.loc[:,train.isnull().any()].columns)  #\n",
    "    trian, test, train_Y = align_data(train,test)\n",
    "    # Drop the target and sk_id_curr from the training data\n",
    "    if 'TARGET' in train:\n",
    "        train = train.drop(columns=['TARGET'])\n",
    "    train = train.drop(columns=['SK_ID_CURR'])  #\n",
    "    test = test.drop(columns=['SK_ID_CURR'])  #\n",
    "    \n",
    "    model, preds = baseline_log_reg(train, train_Y, test, save_path=model_save_path)\n",
    "    create_and_save_submission(test_data, preds, save_path=predications_save_path)\n",
    "\n",
    "    \n",
    "# log_reg_knn_imputation(train_data.copy(), test_data.copy(),\n",
    "#                        model_save_path=\"../models/knn_imputation_top_corr_log_reg1.pickle\",\n",
    "#                        predications_save_path='../test_predictions/knn_imputation_top_corr_log_reg1.csv',\n",
    "#                        n_jobs=10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "# import statsmodels.api as sm\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import Isomap\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# def log_reg_pca_knn_imputation(feature):\n",
    "#     col = app_train[[feature]]\n",
    "\n",
    "#     not_null_mask = app_train[feature].notna() # true if not nan = mask to get the values to train on \n",
    "#     train_x = app_train[not_null_mask] # get all top corr cols (minus target) which have a ext_source_3 value\n",
    "# #     train_x = train_x.drop(columns = [chosen_col, 'SK_ID_CURR', \"TARGET\"])\n",
    "#     train_x = train_x.drop(columns = [chosen_col])\n",
    "#     train_y = app_train[not_null_mask][[chosen_col]] \n",
    "\n",
    "#     # get the rows which require imputation\n",
    "#     test_x = app_train[app_train[chosen_col].isna()] #only null values\n",
    "#     test_x = test_x.drop(columns = [chosen_col, 'TARGET'])\n",
    "\n",
    "#     train_x, test_x, feats_names = normalise_and_impute(train_x, test_x)\n",
    "# #     # Median imputation of missing values\n",
    "# #     imputer = Imputer(strategy = 'mean')\n",
    "# #     imputer.fit(train_x)\n",
    "# #     train_x = imputer.transform(train_x)\n",
    "# #     test_x = imputer.transform(test_x)\n",
    "\n",
    "# #     # Scale each feature to 0-1\n",
    "# #     scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "# #     scaler.fit(train_x)\n",
    "# #     train_x = scaler.transform(train_x)\n",
    "# #     test_x = scaler.transform(test_x)\n",
    "\n",
    "#     pca = PCA(n_components=30, svd_solver='full')\n",
    "#     pca.fit(train_x)\n",
    "#     train_x = pca.transform(train_x)\n",
    "#     test_x = pca.transform(test_x)\n",
    "#     print(\"Captured variance\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "\n",
    "#     # train and predict\n",
    "#     neigh = KNeighborsRegressor(n_neighbors=10)\n",
    "#     neigh.fit(train_x,train_y)\n",
    "#     y_test = neigh.predict(test_x)\n",
    "\n",
    "#     # get the column to update nulls\n",
    "#     filled_col = app_train[[chosen_col]]\n",
    "#     filled_col[filled_col[chosen_col].isna()] = y_test\n",
    "\n",
    "\n",
    "#     col = \"EXT_SOURCE_3\"\n",
    "#     # Drop the target from the training data\n",
    "#     data = app_train.copy()\n",
    "#     data = data[[col,\"TARGET\"]]\n",
    "#     data[col] = filled_col\n",
    "\n",
    "#     cols_to_keep = [\"TARGET\", col]\n",
    "#     data[\"intercept\"] = 1.0\n",
    "#     train_cols = data.columns[1:]\n",
    "#     logit = sm.Logit(data[\"TARGET\"], data[col])\n",
    "#     result = logit.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "# # import statsmodels.api as sm\n",
    "# # from sklearn.decomposition import PCA\n",
    "# # from sklearn.manifold import Isomap\n",
    "# # from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "# def pca_knn_imputation(feature, train, test, train_filled, test_filled,n_jobs):\n",
    "#     top_corrs = top_correlations_from_all(feature, 15).keys()\n",
    "    \n",
    "#     # create training data from columns with values\n",
    "#     not_null_mask = train[feature].notna()  # true if not nan = mask to get the values to train on\n",
    "#     knn_train_x = train_filled[not_null_mask][top_corrs] # get all top corr cols (minus target) which have a feature value\n",
    "#     knn_train_y = train_filled[not_null_mask][[feature]]\n",
    "    \n",
    "#     # get the rows which require to be imputed for the features\n",
    "#     knn_test_x = train_filled[train[feature].isna()][top_corrs]  # only null values\n",
    "#     actual_test_x = test_filled[test[feature].isna()][top_corrs]  # only null values\n",
    "    \n",
    "#     pca = PCA(n_components=30, svd_solver='full')\n",
    "#     pca.fit(knn_train_x)\n",
    "#     train_x = pca.transform(knn_train_x)\n",
    "#     test_x = pca.transform(knn_train_y)\n",
    "#     test_x = pca.transform(actual_test_x)\n",
    "#     print(\"Captured variance\", pca.explained_variance_ratio_.sum())\n",
    "\n",
    "#     # train and predict\n",
    "#     filename = feature.replace(':', '')\n",
    "#     fileaname = feature.replace('/', '')\n",
    "#     print(\"DOING Feature \", feature)\n",
    "#     try:\n",
    "#         neigh = load_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\")\n",
    "#         print(\"loaded knn\")\n",
    "#     except FileNotFoundError:\n",
    "#     #     neigh = KNeighborsRegressor(n_neighbors=75)\n",
    "#         model = KNeighborsRegressor()\n",
    "#         n_neighbors = range(10,200)\n",
    "#         hyperparameters = dict(n_neighbors=n_neighbors)\n",
    "#         neigh = RandomizedSearchCV(model, hyperparameters, n_iter=7, cv=3, verbose=3, n_jobs=n_jobs)\n",
    "#         neigh.fit(knn_train_x, knn_train_y)\n",
    "#         save_pickle(\"../models/knn_corrs_rs_clf/\"+filename+\".pickle\", neigh)\n",
    "        \n",
    "#     print('Best n_neigbors:', neigh.best_estimator_.get_params()['n_neighbors'])\n",
    "#     knn_test_y = neigh.predict(knn_test_x)\n",
    "#     actual_test_y = neigh.predict(actual_test_x)\n",
    "    \n",
    "#     # get the column to update nulls\n",
    "#     train_imputed_col = train[[feature]]\n",
    "#     train_imputed_col[train_imputed_col[feature].isna()] = knn_test_y\n",
    "    \n",
    "#     # get the column to update nulls for app_test\n",
    "#     test_imputed_col = test[[feature]]\n",
    "#     test_imputed_col[test_imputed_col[feature].isna()] = actual_test_y\n",
    "    \n",
    "#     return train_imputed_col, test_imputed_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_corr_knn_imputation(top_corrs, feature: str, app_train, app_test):\n",
    "# #     top_corrs = get_top_correlations(train_data, feature, n=10)  # TODO -> uncomment once working\n",
    "# #     top_corrs = load_pickle('../models/correlations/'+feature+'.pickle', n=10)\n",
    "# #     top_corrs = top_correlations_from_all(top_corrs)\n",
    "    \n",
    "#     # create training data from columns with values\n",
    "#     not_null_mask = app_train[feature].notna()  # true if not nan = mask to get the values to train on\n",
    "#     train_x = app_train[not_null_mask][top_corrs]  # get all top corr cols (minus target) which have a ext_source_3 value\n",
    "#     train_y = app_train[not_null_mask][[feature]]\n",
    "\n",
    "#     # imputation of missing values for the other columns\n",
    "#     imputer = Imputer(strategy='mean')\n",
    "#     train_x = imputer.fit_transform(train_x)\n",
    "\n",
    "#     # get the rows which require imputation for the rest of the columns \n",
    "#     test_x = app_train[app_train[feature].isna()][top_corrs]  # only null values\n",
    "#     imputer = Imputer(strategy='mean')\n",
    "#     test_x = imputer.fit_transform(test_x)\n",
    "\n",
    "#     # as above but for the app_test data\n",
    "#     app_test_x = app_test[app_test[feature].isna()][top_corrs]  # only null values\n",
    "#     imputer = Imputer(strategy='mean')\n",
    "#     app_test_x = imputer.fit_transform(app_test_x)\n",
    "\n",
    "#     # train and predict\n",
    "#     neigh = KNeighborsRegressor(n_neighbors=150)\n",
    "#     neigh.fit(train_x, train_y)\n",
    "#     y_test = neigh.predict(test_x)\n",
    "#     y_app_test = neigh.predict(app_test_x)\n",
    "\n",
    "#     # get the column to update nulls\n",
    "#     filled_col = app_train[[feature]]\n",
    "#     filled_col[filled_col[feature].isna()] = y_test\n",
    "#     # fill column\n",
    "#     data = app_train.copy()\n",
    "#     data[feature] = filled_col\n",
    "\n",
    "#     # get the column to update nulls for app_test\n",
    "#     filled_col_app_test = app_test[[feature]]\n",
    "#     filled_col_app_test[filled_col_app_test[feature].isna()] = y_app_test\n",
    "#     # fill column\n",
    "#     data_app_test = app_test.copy()\n",
    "#     data_app_test[feature] = filled_col_app_test\n",
    "    \n",
    "#     #calc aic score from single col imputation\n",
    "#     imputed_col_aic(data, feature)\n",
    "    \n",
    "#     return data, data_app_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_reg_knn_imputation(top_corrs, feat_name, app_train, app_test, model_save_path, predications_save_path):\n",
    "# #     top_corrs = get_top_correlations(train_data, 'EXT_SOURCE_3', n=15)\n",
    "#     train_Y = app_train['TARGET']\n",
    "#     train_X, test_X = top_corr_knn_imputation(top_corrs, feat_name, app_train, app_test)\n",
    "#     train_X, test_X, feat_names = normalise_and_impute(train_X, test_X,\n",
    "#                                                        impute_strategy='mean')\n",
    "\n",
    "#     model, preds = baseline_log_reg(train_X, train_Y, test_X, save_path=model_save_path)\n",
    "#     create_and_save_submission(test_data, preds, save_path=predications_save_path)\n",
    "\n",
    "    \n",
    "# log_reg_knn_imputation(top_corrs, 'EXT_SOURCE_3', train_data, test_data,\n",
    "#                        model_save_path=\"../models/test.pickle\",\n",
    "#                        predications_save_path='../test_predictions/test.csv') \n",
    "# # test score -> 0.67843 (using baseline params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
